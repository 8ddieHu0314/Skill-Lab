# v0.3.0: Failure Analysis & Auto-Fix

**Status:** Planned

## Overview

Intelligent failure analysis for trigger tests with actionable fix suggestions and interactive auto-fix capability, enabling a feedback loop where users can improve their skills based on test results.

## Features

### Failure Analysis
- Rule-based analysis explaining WHY tests failed
- 8 detection rules for false positives and false negatives
- Keyword extraction and pattern matching
- Actionable fix suggestions with confidence levels

### Interactive Auto-Fix
- Post-report prompt to fix skill description or test expectations
- Deterministic template-based fixes OR LLM-generated fixes (TBD)
- Re-run tests after applying fixes

### Trace Analysis (Carried from previous plan)
- 5 trace check types: command_presence, file_creation, event_sequence, loop_detection, efficiency
- YAML-defined checks per skill
- TraceCheckRegistry with @register_trace_handler decorator

### Carried from v0.2.0
- Codex CLI runtime adapter
- Given/When/Then DSL test format (`tests/scenarios.yaml`)

## CLI Commands

```bash
# Trigger testing with failure analysis (default: full output)
sklab trigger ./my-skill                    # All tests + analysis + all suggestions
sklab trigger ./my-skill --quiet            # Only show failed tests, no analysis (CI mode)
sklab trigger ./my-skill -q                 # Short form of --quiet

# Trigger testing with runtime selection
sklab trigger ./my-skill --runtime codex    # Use Codex CLI
sklab trigger ./my-skill --runtime claude   # Use Claude CLI (default)

# Trace evaluation
sklab eval-trace ./my-skill --trace ./execution.jsonl
```

## Output Modes

### Default Mode (Full Output)

Shows all tests, analysis for failures, and all suggestions:

```
$ sklab trigger ./my-skill

Test Results:
┌──────────────┬────────────┬────────┐
│ Test         │ Type       │ Status │
├──────────────┼────────────┼────────┤
│ explicit-1   │ explicit   │ PASS   │
│ implicit-1   │ implicit   │ PASS   │
│ negative-5   │ negative   │ FAIL   │
└──────────────┴────────────┴────────┘

Failure Analysis:

negative-5: FAIL (expected no_trigger, got trigger)

  Analysis: Triggered because prompt contains keywords matching skill
            description: "commit". However, prompt asks to EXECUTE
            a commit, not DRAFT a message.

  Keywords: commit, changes, repository

  Suggestions:
    1. [description] Add exclusion clause clarifying when NOT to use (high)
       Do NOT use when user asks to execute/run commands.
    2. [test] Consider changing expectation to 'trigger' (medium)
```

### Quiet Mode (`--quiet/-q`)

Shows only failed tests, no analysis (for CI/scripting):

```
$ sklab trigger ./my-skill --quiet

1 test failed:
  - negative-5: expected no_trigger, got trigger
```

## Detection Rules

### False Positive Rules (triggered when shouldn't)

| Rule | Pattern | Detection Logic |
|------|---------|-----------------|
| `FP-1` | Keyword overlap | `prompt_keywords ∩ desc_keywords` is non-empty |
| `FP-2` | Execution without drafting | Prompt has execution verb but no drafting verb |
| `FP-3` | Inline content provided | Prompt contains `-m 'message'` or similar |
| `FP-4` | Informational query | Prompt starts with `how do I`, `what is`, etc. |

### False Negative Rules (didn't trigger when should)

| Rule | Pattern | Detection Logic |
|------|---------|-----------------|
| `FN-1` | Missing keywords | `prompt_keywords - desc_keywords` has important words |
| `FN-2` | No keyword overlap | `prompt_keywords ∩ desc_keywords` is empty |
| `FN-3` | Synonym gap | Prompt uses known synonyms not in description |
| `FN-4` | Too indirect | Implicit/contextual test has zero overlap |

## Interactive Auto-Fix UX

After showing the failure report, prompt user for action:

```
$ sklab trigger ./my-skill

[shows report]

negative-5: FAIL (expected no_trigger, got trigger)
  Analysis: Prompt asks to execute, not draft...

━━━ Fix Options ━━━
  [1] Skill: Add "Do NOT use when user asks to execute commands"
  [2] Test: Change negative-5 expectation to 'trigger'
  [s] Skip this failure
  [q] Quit

Choice [1/2/s/q]: 2
✓ Updated tests/triggers.yaml

Re-run tests? [Y/n]: y
[re-runs tests]
All 17 tests passed!
```

## Auto-Fix Implementation (TBD)

Two approaches under consideration:

| Approach | Description | Pros | Cons |
|----------|-------------|------|------|
| **Deterministic** | Template-based fixes mapped to detection rules | Fast, free, predictable | Rigid, may not fit all cases |
| **LLM-based** | Send context to LLM to generate fix | Flexible, semantic understanding | API cost, latency |

### Deterministic Fix Templates

| Rule | Auto-Fix Template |
|------|-------------------|
| `FP-1` | Append exclusion clause to description |
| `FP-2` | Add "Do NOT use for execution/running commands" |
| `FP-3` | Add "Do NOT use when user provides content inline" |
| `FP-4` | Add "Do NOT use for informational questions" |
| `FN-1` | Append missing keywords to description |
| `FN-2` | Broaden description with synonyms |
| `FN-3` | Add synonym mappings |
| `FN-4` | Change test expectation to match behavior |

## Trace Check Definition

```yaml
# tests/trace_checks.yaml
checks:
  - id: npm-install-ran
    type: command_presence
    pattern: "npm install"

  - id: package-json-created
    type: file_creation
    path: "package.json"

  - id: correct-sequence
    type: event_sequence
    sequence: ["npm init", "npm install", "npm run build"]

  - id: no-excessive-retries
    type: loop_detection
    max_retries: 3

  - id: command-count-limit
    type: efficiency
    max_commands: 20
```

## Given/When/Then DSL Test Format

Advanced trigger test definition with structured scenarios (`tests/scenarios.yaml`):

```yaml
skill: my-skill
scenarios:
  - name: "Direct invocation"
    given:
      - skill: my-skill
      - runtime: codex
    when:
      - prompt: "$my-skill do something"
      - trigger_type: explicit
    then:
      - skill_triggered: true
      - exit_code: 0

  - name: "Implicit scenario"
    given:
      - skill: my-skill
    when:
      - prompt: "I need to scaffold a new React application"
      - trigger_type: implicit
    then:
      - skill_triggered: true
      - commands_include: ["npm init"]
```

## Architecture

```
src/skill_lab/
├── triggers/
│   ├── failure_analyzer.py     # NEW: Rule-based failure analysis
│   ├── auto_fixer.py           # NEW: Apply fixes to SKILL.md or triggers.yaml
│   ├── test_loader.py
│   ├── trigger_evaluator.py    # MODIFIED: Integrate failure analyzer
│   └── trace_analyzer.py
├── core/
│   └── models.py               # MODIFIED: Add FailureAnalysis, FixSuggestion
├── tracechecks/
│   ├── registry.py             # TraceCheckRegistry
│   ├── trace_check_loader.py   # YAML loader
│   └── handlers/               # Check implementations
│       ├── command_presence.py
│       ├── file_creation.py
│       ├── event_sequence.py
│       ├── loop_detection.py
│       └── efficiency.py
├── parsers/
│   └── trace_parser.py         # JSONL parser
└── evaluators/
    └── trace_evaluator.py      # Orchestrator
```

## Deliverables

### Failure Analysis
- [ ] `FixSuggestion` and `FailureAnalysis` dataclasses in models.py
- [ ] Extend `TriggerResult` with optional `failure_analysis` field
- [ ] `FailureAnalyzer` class with 8 detection rules
- [ ] Keyword extraction and pattern matching logic
- [ ] CLI flag: `--quiet/-q` (default shows full output with analysis)
- [ ] Enhanced `_print_trigger_report()` with analysis display
- [ ] Unit tests for failure analyzer

### Interactive Auto-Fix
- [ ] `AutoFixer` class to apply fixes to SKILL.md or triggers.yaml
- [ ] Interactive prompt after failure report (TTY detection)
- [ ] Fix template system (deterministic approach)
- [ ] Optional: LLM-based fix generation
- [ ] Re-run tests after fix option
- [ ] Unit tests for auto-fixer

### Trace Analysis
- [ ] TraceCheckDefinition, TraceCheckResult, TraceReport models
- [ ] JSONL trace parser
- [ ] TraceCheckRegistry with @register_trace_handler
- [ ] 5 trace check handlers
- [ ] TraceEvaluator orchestrator
- [ ] CLI command: `sklab eval-trace` (unhide)

### Extended Runtimes & Test Formats
- [ ] Codex CLI runtime adapter (full implementation)
- [ ] Given/When/Then DSL test loader (`tests/scenarios.yaml`)
- [ ] Runtime selection via `--runtime` flag

## Verification

```bash
# Failure analysis tests
pytest tests/test_failure_analyzer.py -v
pytest tests/test_auto_fixer.py -v

# Trace analysis tests
pytest tests/test_trace_evaluator.py -v
pytest tests/test_trace_handlers.py -v

# Runtime tests
pytest tests/test_runtimes.py -v

# Manual testing - Failure analysis
sklab trigger ./my-skill                    # Verify full output: all tests + analysis + suggestions
sklab trigger ./my-skill --quiet            # Verify quiet mode: only failed tests, no analysis
sklab trigger ./my-skill -q                 # Verify short flag works

# Manual testing - Auto-fix (interactive)
# 1. Create a skill with a failing test
# 2. Run: sklab trigger ./my-skill
# 3. Verify fix options are shown
# 4. Select a fix option
# 5. Verify file is updated
# 6. Verify re-run option works

# Manual testing - Trace & runtime
sklab eval-trace ./my-skill --trace ./execution.jsonl
sklab trigger ./my-skill --runtime codex
```

## LLM-as-Judge: Dynamic Quality Checks

Some spec recommendations require semantic understanding and cannot be validated with static analysis. These checks use an LLM to evaluate quality.

### Why LLM-as-Judge?

The Agent Skills spec includes recommendations like:
- "Should include specific keywords that help agents identify relevant tasks"
- "Should describe both what the skill does and when to use it"

These require understanding context and intent, not just pattern matching.

### Proposed Dynamic Checks

| Check ID | Description | What LLM Evaluates |
|----------|-------------|-------------------|
| `description.keyword-quality` | Description has effective trigger keywords | Are the keywords specific enough? Would an agent correctly identify when to use this skill? |
| `description.semantic-completeness` | Description covers what AND when | Does it explain both the skill's purpose and activation conditions? |
| `content.instruction-clarity` | Body has clear, actionable instructions | Are the instructions unambiguous? Could an agent follow them? |
| `content.example-relevance` | Examples are relevant and helpful | Do the examples demonstrate realistic use cases? |

### CLI Integration

```bash
# Run with LLM-based checks (requires API key)
sklab evaluate ./my-skill --llm-checks

# Run specific LLM check
sklab evaluate ./my-skill --check description.keyword-quality

# Skip LLM checks (default, for speed/cost)
sklab evaluate ./my-skill
```

### Configuration

```yaml
# .skill-lab.yaml or pyproject.toml
[tool.skill-lab]
llm_provider = "anthropic"  # or "openai"
llm_model = "claude-sonnet-4-20250514"
llm_checks_enabled = false  # opt-in
```

### Output Format

```
$ sklab evaluate ./my-skill --llm-checks

Quality Score: 87.5/100
Status: PASS

LLM-Based Checks:
┌─────────────────────────────────┬────────┬─────────────────────────────────────┐
│ Check                           │ Status │ Feedback                            │
├─────────────────────────────────┼────────┼─────────────────────────────────────┤
│ description.keyword-quality     │ PASS   │ Keywords "commit", "message",       │
│                                 │        │ "draft" are specific and relevant   │
├─────────────────────────────────┼────────┼─────────────────────────────────────┤
│ description.semantic-complete   │ WARN   │ Describes WHAT but not clearly WHEN │
│                                 │        │ Suggestion: Add "Use when..." clause│
└─────────────────────────────────┴────────┴─────────────────────────────────────┘
```

### Deliverables (LLM-as-Judge)

- [ ] LLM provider abstraction (Anthropic, OpenAI)
- [ ] `description.keyword-quality` check
- [ ] `description.semantic-completeness` check
- [ ] `content.instruction-clarity` check
- [ ] `content.example-relevance` check
- [ ] CLI flag: `--llm-checks`
- [ ] Configuration for LLM provider/model
- [ ] Caching to avoid redundant API calls
- [ ] Unit tests with mocked LLM responses

---

## Design Decisions (TBD)

1. **Deterministic vs LLM-based fixing** - Start with deterministic, add LLM as optional enhancement?
2. **Interactive prompt vs `--fix` flag** - Interactive by default, `--fix=skill` or `--fix=test` for CI?
3. **Handling multiple failures** - Process one at a time? Batch similar fixes?
4. **Re-run behavior** - Auto re-run after each fix, or after all fixes applied?
5. **LLM-as-Judge cost management** - How to balance quality vs API costs? Caching strategy?
