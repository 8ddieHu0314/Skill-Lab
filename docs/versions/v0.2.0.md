# v0.2.0: Trigger Testing

**Status:** Released

## Overview

Test whether skills activate correctly by sending real prompts to LLMs and analyzing execution traces.

## Features

- 4 trigger test types: explicit, implicit, contextual, negative
- Claude CLI runtime adapter (Codex CLI deferred to v0.3.0)
- Simple YAML test definition format
- Trace analysis for skill invocation detection
- Progress spinner showing current test during execution

## Trigger Test Types

| Type | Description | Example |
|------|-------------|---------|
| **Explicit** | Skill named directly with $ prefix | `$create-react-app for a todo list` |
| **Implicit** | Describes exact scenario without naming skill | `I need to scaffold a new React application` |
| **Contextual** | Realistic noisy prompt with domain context | `Building a dashboard, can you set up React?` |
| **Negative** | Should NOT trigger (catches false positives) | `How do I fix this useState hook?` |

## CLI Commands

```bash
sklab trigger ./my-skill                    # Run all trigger tests (uses Claude CLI)
sklab trigger ./my-skill -t explicit        # Filter by trigger type
sklab trigger ./my-skill -f json            # Output as JSON
sklab trigger ./my-skill -o results.json    # Save JSON to file

# Global flags
sklab -v                                    # Show version
sklab -h                                    # Show help
sklab trigger -h                            # Show trigger command help
```

## Prerequisites

Trigger testing requires:
- **Claude CLI**: Install via `npm install -g @anthropic-ai/claude-code`

> **Note:** Codex CLI support is planned for v0.3.0.

## Test Definition Format

Tests are defined in `tests/triggers.yaml`:

```yaml
skill: my-skill
test_cases:
  - id: explicit-1
    name: "Direct skill invocation"
    type: explicit
    prompt: "$my-skill do something"
    expected: trigger

  - id: implicit-1
    name: "Scenario description"
    type: implicit
    prompt: "I need to scaffold a new React application"
    expected: trigger

  - id: negative-1
    name: "Should not trigger"
    type: negative
    prompt: "How do I fix this useState hook?"
    expected: no_trigger
```

### Test Case Fields

| Field | Required | Description |
|-------|----------|-------------|
| `id` | Yes | Unique identifier for the test |
| `name` | No | Human-readable test name |
| `type` | Yes | Trigger type: explicit, implicit, contextual, negative |
| `prompt` | Yes | The prompt to send to the LLM |
| `expected` | Yes | Expected result: `trigger` or `no_trigger` |

> **Note:** Given/When/Then DSL format (`tests/scenarios.yaml`) is planned for v0.3.0.

## Architecture

```
src/skill_lab/
├── triggers/
│   ├── test_loader.py         # YAML test loading
│   ├── trigger_evaluator.py   # Orchestrates tests (with progress callback)
│   └── trace_analyzer.py      # Skill invocation detection
└── runtimes/
    ├── base.py                # RuntimeAdapter ABC
    ├── codex_runtime.py       # Codex CLI adapter (stub, v0.3.0)
    └── claude_runtime.py      # Claude CLI adapter
```

## Important Note: Execution Environment

**Commands execute on the user's local machine**, not in a sandbox. Skills can:
- Install packages (`npm install`, `pip install`)
- Create, modify, delete files
- Run arbitrary shell commands

Docker sandboxing is planned for v0.4.0.

## Spec Compliance Improvements

Audit of static checks against the [Agent Skills Specification](https://agentskills.io/specification) with fixes and additions.

### New Check: `structure.standard-frontmatter-fields`

Warns when frontmatter contains fields not defined in the official spec. This helps catch custom/non-standard fields that may cause unexpected behavior across different agent implementations.

**Spec-defined fields:**
- `name` (required)
- `description` (required)
- `license` (optional)
- `compatibility` (optional)
- `metadata` (optional)
- `allowed-tools` (optional/experimental)

**Example warning:**
```
⚠ structure.standard-frontmatter-fields: Non-standard frontmatter fields:
  agent, argument-hint, context, disable-model-invocation
```

### Fixed: `naming.format`

**Before:** Required names to start with a letter (`^[a-z]`)
**After:** Allows names to start with numbers (`^[a-z0-9]`)

Per spec, names must only "not start or end with a hyphen". Names like `30daysresearch`, `123`, `1a-test` are now valid.

### Fixed: `frontmatter.compatibility-length`

**Before:** Only checked max length (500 chars)
**After:** Also checks for empty/whitespace values

Per spec, compatibility must be "1-500 characters if provided". Empty `compatibility: ""` now fails.

### Spec Compliance Summary

| Field | Checks Implemented |
|-------|-------------------|
| `name` | Length (1-64), format (lowercase alphanumeric + hyphens), no start/end hyphen, no consecutive `--`, matches directory |
| `description` | Required, non-empty, max 1024 chars, trigger words (INFO) |
| `compatibility` | 1-500 chars if provided, must be string |
| `metadata` | String-to-string mapping validation |
| `allowed-tools` | Space-delimited string validation |

### New Check: `frontmatter.license-format`

Validates the optional `license` field when present. Must be a non-empty string. Passes if the field is absent.

- **Severity:** WARNING
- **Spec-required:** No (quality suggestion)

### Refactor: Schema-Based Frontmatter Validation

Replaced 8 hand-written check classes with declarative `FieldRule` data in a new `schema.py` module. A generic `_validate_rule()` engine interprets rules and produces identical `CheckResult` output.

**What changed:**
- `frontmatter.py` deleted — all 3 checks moved to `schema.py`
- `naming.py` trimmed from 3 checks to 1 (kept `naming.matches-directory`)
- `description.py` trimmed from 4 checks to 1 (kept `description.includes-triggers`)
- `schema.py` created with 9 declarative `FieldRule` definitions

**Why:** When the Agent Skills spec changes, validation updates are now a data edit (add/modify a `FieldRule`) instead of writing a new Python class.

**Check count:** 18 (v0.1.0) → 20 (added `structure.standard-frontmatter-fields` + `frontmatter.license-format`)

### Deferred to Future Release: Dynamic Checks (LLM-as-Judge)

Some spec recommendations require semantic understanding and cannot be validated statically:

- **Description quality**: "Should include specific keywords that help agents identify relevant tasks"
- **Description completeness**: "Should describe both what the skill does and when to use it"

These will be implemented in a future release using LLM-as-judge evaluation.

---

## Deliverables

- [x] TriggerTestCase, TriggerResult, TriggerReport models
- [x] YAML test case loader (simple format)
- [x] RuntimeAdapter abstract base class
- [x] Claude CLI runtime adapter
- [x] TraceAnalyzer for skill invocation detection
- [x] TriggerEvaluator orchestrator
- [x] CLI command: `sklab trigger`
- [x] Progress spinner with current test display
- [x] CLI flags: `-v`/`--version`, `-h`/`--help`
- [x] New check: `structure.standard-frontmatter-fields`
- [x] Fixed: `naming.format` to allow names starting with numbers
- [x] Fixed: `frontmatter.compatibility-length` to check for empty values
- [x] Tests for all spec compliance changes
- [x] New check: `frontmatter.license-format`
- [x] Refactor: schema-based frontmatter validation (`schema.py` replaces `frontmatter.py`)
- [x] Documentation updates
- [x] Release to PyPI

### Deferred to v0.3.0

- [ ] Codex CLI runtime adapter
- [ ] Given/When/Then DSL format (`tests/scenarios.yaml`)
- [ ] `sklab eval-trace` command (hidden in v0.2.0)

## Verification

```bash
# Unit tests
pytest tests/test_triggers.py -v
pytest tests/test_runtimes.py -v

# Manual testing (requires Claude CLI installed)
sklab trigger ./tests/fixtures/skills/testing-features
```
